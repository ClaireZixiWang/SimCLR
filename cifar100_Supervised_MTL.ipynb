{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR100_Coarse_Fine_MTL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaireZixiWang/SimCLR/blob/master/CIFAR100_Supervised_MTL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n"
      ],
      "metadata": {
        "id": "NM42ZuLUb_fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch"
      ],
      "metadata": {
        "id": "O-m5tv4Wfn-m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "qDQObJJn02T2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0003\n",
        "BATCH_SIZE = 256\n",
        "weight_decay = 1e-4\n",
        "PATH = '/content/drive/MyDrive/resnet18'"
      ],
      "metadata": {
        "id": "zEY0LRHmIZ7h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXAeMMV_NHJC",
        "outputId": "e8853e17-f158-4c1d-bd4e-22639ec160e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Borrowed from https://github.com/ryanchankh/cifar100coarse\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "\n",
        "class CIFAR100TwoLabels(CIFAR100):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        super(CIFAR100TwoLabels, self).__init__(root, train, transform, target_transform, download)\n",
        "\n",
        "        # update labels\n",
        "        coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
        "                                   3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
        "                                   6, 11,  5, 10,  7,  6, 13, 15,  3, 15, \n",
        "                                   0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
        "                                   5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
        "                                   16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
        "                                   10, 3,  2, 12, 12, 16, 12,  1,  9, 19, \n",
        "                                   2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
        "                                  16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
        "                                  18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
        "        self.targets = np.array([self.targets, coarse_labels[self.targets]]).transpose()\n",
        "\n",
        "        # update classes\n",
        "        self.classes = [['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
        "                        ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
        "                        ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
        "                        ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
        "                        ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
        "                        ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
        "                        ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
        "                        ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
        "                        ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
        "                        ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
        "                        ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
        "                        ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
        "                        ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
        "                        ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
        "                        ['baby', 'boy', 'girl', 'man', 'woman'],\n",
        "                        ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
        "                        ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
        "                        ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
        "                        ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
        "                        ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']]"
      ],
      "metadata": {
        "id": "6-NzpAOFcPo9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "yRfFvlcJKr-z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CIFAR100TwoLabels(root='./data', \n",
        "                                  train=True, \n",
        "                                  transform=transform, \n",
        "                                  download=True)\n",
        "test_dataset = CIFAR100TwoLabels(root='./data', \n",
        "                                 train=False, \n",
        "                                 transform=transform, \n",
        "                                 download=True)\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=BATCH_SIZE, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=BATCH_SIZE, \n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pvqPF4Ucdqs",
        "outputId": "2edae3ad-ba38-4134-87b1-d069dedd4b4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet18 with two prediction head"
      ],
      "metadata": {
        "id": "OM88l6d-Fx6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetTwoPrediction(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResNetTwoPrediction, self).__init__()\n",
        "    self.backbone = models.resnet18(pretrained=False)\n",
        "    dim = self.backbone.fc.in_features\n",
        "    # print(dim)\n",
        "    self.backbone.fc = nn.Identity() # place holder for the fc layer\n",
        "    self.fine = nn.Sequential(nn.Linear(dim, dim), \n",
        "                                       nn.ReLU(), \n",
        "                                       nn.Linear(in_features=512, out_features=100, bias=True))\n",
        "    self.coarse = nn.Sequential(nn.Linear(dim, dim), \n",
        "                                         nn.ReLU(), \n",
        "                                         nn.Linear(in_features=512, out_features=20, bias=True))\n",
        "\n",
        "  def forward(self, x):\n",
        "    base_representation = self.backbone(x)\n",
        "    fine_out = self.fine(base_representation)\n",
        "    coarse_out = self.coarse(base_representation)\n",
        "    return fine_out, coarse_out\n"
      ],
      "metadata": {
        "id": "RIp1ehku-q-A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetTwoPrediction().to(device)\n"
      ],
      "metadata": {
        "id": "wdR3wyzYtniN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetTwoPrediction().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n",
        "                                                           last_epoch=-1)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    labels = labels.transpose(0,1)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    fine_out, coarse_out = model(images)\n",
        "    fine_loss = criterion(fine_out, labels[0])\n",
        "    coarse_loss = criterion(coarse_out, labels[1])\n",
        "    # print(fine_loss, coarse_loss)\n",
        "    # Multi task loss\n",
        "    loss = 0.5 * coarse_loss + 0.5 * fine_loss\n",
        "    # print(loss)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print (\"Epoch [{}/{}], Step [{}/{}], Fine Loss: {:.4f}, Coarse Loss: {:.4f},  Total Loss: {:.4f}\"\n",
        "                .format(epoch+1, num_epochs, i+1, total_step, fine_loss.item(), coarse_loss.item(), loss.item()))\n",
        "\n",
        "    # Decay learning rate\n",
        "  if epoch >= 10:\n",
        "    scheduler.step()                                                           \n",
        "\n",
        "  if epoch %10 == 0:\n",
        "    checkpoint_name = 'checkpoint_resnet18-sup-mtl_{:04d}.pth.tar'.format(epoch)\n",
        "    torch.save(model.state_dict(), PATH+'/'+checkpoint_name)\n",
        "\n",
        "checkpoint_name = 'checkpoint_resnet18-sup-mtl_{:04d}.pth.tar'.format(num_epochs)\n",
        "torch.save(model.state_dict(), PATH+'/'+checkpoint_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lijmj26sH0o4",
        "outputId": "fff1688e-5ec5-4063-d899-70a2b9426d62"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [100/196], Fine Loss: 3.6724, Coarse Loss: 2.2666,  Total Loss: 2.9695\n",
            "Epoch [2/100], Step [100/196], Fine Loss: 3.3861, Coarse Loss: 2.1717,  Total Loss: 2.7789\n",
            "Epoch [3/100], Step [100/196], Fine Loss: 2.6449, Coarse Loss: 1.7292,  Total Loss: 2.1871\n",
            "Epoch [4/100], Step [100/196], Fine Loss: 2.7102, Coarse Loss: 1.7171,  Total Loss: 2.2137\n",
            "Epoch [5/100], Step [100/196], Fine Loss: 2.4895, Coarse Loss: 1.5964,  Total Loss: 2.0430\n",
            "Epoch [6/100], Step [100/196], Fine Loss: 2.4389, Coarse Loss: 1.6149,  Total Loss: 2.0269\n",
            "Epoch [7/100], Step [100/196], Fine Loss: 2.5327, Coarse Loss: 1.6674,  Total Loss: 2.1000\n",
            "Epoch [8/100], Step [100/196], Fine Loss: 2.2877, Coarse Loss: 1.4547,  Total Loss: 1.8712\n",
            "Epoch [9/100], Step [100/196], Fine Loss: 2.2837, Coarse Loss: 1.4548,  Total Loss: 1.8693\n",
            "Epoch [10/100], Step [100/196], Fine Loss: 2.0671, Coarse Loss: 1.2988,  Total Loss: 1.6830\n",
            "Epoch [11/100], Step [100/196], Fine Loss: 2.0979, Coarse Loss: 1.2715,  Total Loss: 1.6847\n",
            "Epoch [12/100], Step [100/196], Fine Loss: 1.8420, Coarse Loss: 1.1852,  Total Loss: 1.5136\n",
            "Epoch [13/100], Step [100/196], Fine Loss: 1.8912, Coarse Loss: 1.2420,  Total Loss: 1.5666\n",
            "Epoch [14/100], Step [100/196], Fine Loss: 1.8023, Coarse Loss: 1.0786,  Total Loss: 1.4405\n",
            "Epoch [15/100], Step [100/196], Fine Loss: 1.7476, Coarse Loss: 1.0182,  Total Loss: 1.3829\n",
            "Epoch [16/100], Step [100/196], Fine Loss: 1.4830, Coarse Loss: 0.8579,  Total Loss: 1.1705\n",
            "Epoch [17/100], Step [100/196], Fine Loss: 1.9283, Coarse Loss: 1.1510,  Total Loss: 1.5397\n",
            "Epoch [18/100], Step [100/196], Fine Loss: 1.5682, Coarse Loss: 0.9421,  Total Loss: 1.2551\n",
            "Epoch [19/100], Step [100/196], Fine Loss: 1.7435, Coarse Loss: 1.1436,  Total Loss: 1.4435\n",
            "Epoch [20/100], Step [100/196], Fine Loss: 1.5662, Coarse Loss: 1.0053,  Total Loss: 1.2857\n",
            "Epoch [21/100], Step [100/196], Fine Loss: 1.4841, Coarse Loss: 0.9446,  Total Loss: 1.2143\n",
            "Epoch [22/100], Step [100/196], Fine Loss: 1.3965, Coarse Loss: 0.7611,  Total Loss: 1.0788\n",
            "Epoch [23/100], Step [100/196], Fine Loss: 1.5668, Coarse Loss: 0.9637,  Total Loss: 1.2652\n",
            "Epoch [24/100], Step [100/196], Fine Loss: 1.3489, Coarse Loss: 0.8514,  Total Loss: 1.1001\n",
            "Epoch [25/100], Step [100/196], Fine Loss: 1.4604, Coarse Loss: 0.9800,  Total Loss: 1.2202\n",
            "Epoch [26/100], Step [100/196], Fine Loss: 1.2787, Coarse Loss: 0.7507,  Total Loss: 1.0147\n",
            "Epoch [27/100], Step [100/196], Fine Loss: 1.2001, Coarse Loss: 0.7105,  Total Loss: 0.9553\n",
            "Epoch [28/100], Step [100/196], Fine Loss: 1.1899, Coarse Loss: 0.7216,  Total Loss: 0.9558\n",
            "Epoch [29/100], Step [100/196], Fine Loss: 1.0860, Coarse Loss: 0.6660,  Total Loss: 0.8760\n",
            "Epoch [30/100], Step [100/196], Fine Loss: 1.0152, Coarse Loss: 0.5770,  Total Loss: 0.7961\n",
            "Epoch [31/100], Step [100/196], Fine Loss: 1.0192, Coarse Loss: 0.6203,  Total Loss: 0.8198\n",
            "Epoch [32/100], Step [100/196], Fine Loss: 0.9945, Coarse Loss: 0.5676,  Total Loss: 0.7811\n",
            "Epoch [33/100], Step [100/196], Fine Loss: 0.8472, Coarse Loss: 0.5364,  Total Loss: 0.6918\n",
            "Epoch [34/100], Step [100/196], Fine Loss: 0.8748, Coarse Loss: 0.4921,  Total Loss: 0.6835\n",
            "Epoch [35/100], Step [100/196], Fine Loss: 0.9240, Coarse Loss: 0.5406,  Total Loss: 0.7323\n",
            "Epoch [36/100], Step [100/196], Fine Loss: 0.8474, Coarse Loss: 0.4669,  Total Loss: 0.6571\n",
            "Epoch [37/100], Step [100/196], Fine Loss: 0.7849, Coarse Loss: 0.4945,  Total Loss: 0.6397\n",
            "Epoch [38/100], Step [100/196], Fine Loss: 0.9298, Coarse Loss: 0.6155,  Total Loss: 0.7726\n",
            "Epoch [39/100], Step [100/196], Fine Loss: 0.7044, Coarse Loss: 0.4408,  Total Loss: 0.5726\n",
            "Epoch [40/100], Step [100/196], Fine Loss: 0.8116, Coarse Loss: 0.4925,  Total Loss: 0.6521\n",
            "Epoch [41/100], Step [100/196], Fine Loss: 0.6835, Coarse Loss: 0.3284,  Total Loss: 0.5059\n",
            "Epoch [42/100], Step [100/196], Fine Loss: 0.7122, Coarse Loss: 0.3558,  Total Loss: 0.5340\n",
            "Epoch [43/100], Step [100/196], Fine Loss: 0.7107, Coarse Loss: 0.3808,  Total Loss: 0.5458\n",
            "Epoch [44/100], Step [100/196], Fine Loss: 0.6462, Coarse Loss: 0.3189,  Total Loss: 0.4826\n",
            "Epoch [45/100], Step [100/196], Fine Loss: 0.5542, Coarse Loss: 0.3728,  Total Loss: 0.4635\n",
            "Epoch [46/100], Step [100/196], Fine Loss: 0.6482, Coarse Loss: 0.4201,  Total Loss: 0.5341\n",
            "Epoch [47/100], Step [100/196], Fine Loss: 0.5990, Coarse Loss: 0.2837,  Total Loss: 0.4414\n",
            "Epoch [48/100], Step [100/196], Fine Loss: 0.5505, Coarse Loss: 0.2836,  Total Loss: 0.4170\n",
            "Epoch [49/100], Step [100/196], Fine Loss: 0.6020, Coarse Loss: 0.3314,  Total Loss: 0.4667\n",
            "Epoch [50/100], Step [100/196], Fine Loss: 0.4633, Coarse Loss: 0.2221,  Total Loss: 0.3427\n",
            "Epoch [51/100], Step [100/196], Fine Loss: 0.4691, Coarse Loss: 0.2856,  Total Loss: 0.3774\n",
            "Epoch [52/100], Step [100/196], Fine Loss: 0.5246, Coarse Loss: 0.2814,  Total Loss: 0.4030\n",
            "Epoch [53/100], Step [100/196], Fine Loss: 0.3777, Coarse Loss: 0.2173,  Total Loss: 0.2975\n",
            "Epoch [54/100], Step [100/196], Fine Loss: 0.4250, Coarse Loss: 0.3053,  Total Loss: 0.3651\n",
            "Epoch [55/100], Step [100/196], Fine Loss: 0.3776, Coarse Loss: 0.2023,  Total Loss: 0.2899\n",
            "Epoch [56/100], Step [100/196], Fine Loss: 0.3952, Coarse Loss: 0.2361,  Total Loss: 0.3156\n",
            "Epoch [57/100], Step [100/196], Fine Loss: 0.4215, Coarse Loss: 0.2116,  Total Loss: 0.3165\n",
            "Epoch [58/100], Step [100/196], Fine Loss: 0.3872, Coarse Loss: 0.2275,  Total Loss: 0.3073\n",
            "Epoch [59/100], Step [100/196], Fine Loss: 0.3774, Coarse Loss: 0.1783,  Total Loss: 0.2779\n",
            "Epoch [60/100], Step [100/196], Fine Loss: 0.3213, Coarse Loss: 0.1464,  Total Loss: 0.2338\n",
            "Epoch [61/100], Step [100/196], Fine Loss: 0.2804, Coarse Loss: 0.1732,  Total Loss: 0.2268\n",
            "Epoch [62/100], Step [100/196], Fine Loss: 0.3776, Coarse Loss: 0.1960,  Total Loss: 0.2868\n",
            "Epoch [63/100], Step [100/196], Fine Loss: 0.2863, Coarse Loss: 0.1629,  Total Loss: 0.2246\n",
            "Epoch [64/100], Step [100/196], Fine Loss: 0.2763, Coarse Loss: 0.1112,  Total Loss: 0.1938\n",
            "Epoch [65/100], Step [100/196], Fine Loss: 0.3151, Coarse Loss: 0.2415,  Total Loss: 0.2783\n",
            "Epoch [66/100], Step [100/196], Fine Loss: 0.2493, Coarse Loss: 0.0894,  Total Loss: 0.1694\n",
            "Epoch [67/100], Step [100/196], Fine Loss: 0.3957, Coarse Loss: 0.1998,  Total Loss: 0.2977\n",
            "Epoch [68/100], Step [100/196], Fine Loss: 0.2720, Coarse Loss: 0.1506,  Total Loss: 0.2113\n",
            "Epoch [69/100], Step [100/196], Fine Loss: 0.2912, Coarse Loss: 0.1580,  Total Loss: 0.2246\n",
            "Epoch [70/100], Step [100/196], Fine Loss: 0.1892, Coarse Loss: 0.0993,  Total Loss: 0.1442\n",
            "Epoch [71/100], Step [100/196], Fine Loss: 0.2287, Coarse Loss: 0.1277,  Total Loss: 0.1782\n",
            "Epoch [72/100], Step [100/196], Fine Loss: 0.2666, Coarse Loss: 0.1639,  Total Loss: 0.2153\n",
            "Epoch [73/100], Step [100/196], Fine Loss: 0.1766, Coarse Loss: 0.1004,  Total Loss: 0.1385\n",
            "Epoch [74/100], Step [100/196], Fine Loss: 0.1679, Coarse Loss: 0.0758,  Total Loss: 0.1218\n",
            "Epoch [75/100], Step [100/196], Fine Loss: 0.2367, Coarse Loss: 0.1211,  Total Loss: 0.1789\n",
            "Epoch [76/100], Step [100/196], Fine Loss: 0.2387, Coarse Loss: 0.1193,  Total Loss: 0.1790\n",
            "Epoch [77/100], Step [100/196], Fine Loss: 0.2367, Coarse Loss: 0.1304,  Total Loss: 0.1836\n",
            "Epoch [78/100], Step [100/196], Fine Loss: 0.2373, Coarse Loss: 0.1572,  Total Loss: 0.1973\n",
            "Epoch [79/100], Step [100/196], Fine Loss: 0.1631, Coarse Loss: 0.1228,  Total Loss: 0.1430\n",
            "Epoch [80/100], Step [100/196], Fine Loss: 0.1782, Coarse Loss: 0.1055,  Total Loss: 0.1418\n",
            "Epoch [81/100], Step [100/196], Fine Loss: 0.1614, Coarse Loss: 0.0931,  Total Loss: 0.1272\n",
            "Epoch [82/100], Step [100/196], Fine Loss: 0.1561, Coarse Loss: 0.1396,  Total Loss: 0.1478\n",
            "Epoch [83/100], Step [100/196], Fine Loss: 0.1572, Coarse Loss: 0.1012,  Total Loss: 0.1292\n",
            "Epoch [84/100], Step [100/196], Fine Loss: 0.1710, Coarse Loss: 0.1350,  Total Loss: 0.1530\n",
            "Epoch [85/100], Step [100/196], Fine Loss: 0.1699, Coarse Loss: 0.1267,  Total Loss: 0.1483\n",
            "Epoch [86/100], Step [100/196], Fine Loss: 0.1367, Coarse Loss: 0.0964,  Total Loss: 0.1166\n",
            "Epoch [87/100], Step [100/196], Fine Loss: 0.1875, Coarse Loss: 0.0963,  Total Loss: 0.1419\n",
            "Epoch [88/100], Step [100/196], Fine Loss: 0.1266, Coarse Loss: 0.0299,  Total Loss: 0.0782\n",
            "Epoch [89/100], Step [100/196], Fine Loss: 0.0834, Coarse Loss: 0.0625,  Total Loss: 0.0729\n",
            "Epoch [90/100], Step [100/196], Fine Loss: 0.1485, Coarse Loss: 0.0542,  Total Loss: 0.1014\n",
            "Epoch [91/100], Step [100/196], Fine Loss: 0.1287, Coarse Loss: 0.0656,  Total Loss: 0.0971\n",
            "Epoch [92/100], Step [100/196], Fine Loss: 0.0791, Coarse Loss: 0.0354,  Total Loss: 0.0573\n",
            "Epoch [93/100], Step [100/196], Fine Loss: 0.0941, Coarse Loss: 0.0648,  Total Loss: 0.0795\n",
            "Epoch [94/100], Step [100/196], Fine Loss: 0.1168, Coarse Loss: 0.0462,  Total Loss: 0.0815\n",
            "Epoch [95/100], Step [100/196], Fine Loss: 0.1409, Coarse Loss: 0.0418,  Total Loss: 0.0914\n",
            "Epoch [96/100], Step [100/196], Fine Loss: 0.0991, Coarse Loss: 0.0644,  Total Loss: 0.0818\n",
            "Epoch [97/100], Step [100/196], Fine Loss: 0.1461, Coarse Loss: 0.0547,  Total Loss: 0.1004\n",
            "Epoch [98/100], Step [100/196], Fine Loss: 0.1492, Coarse Loss: 0.0938,  Total Loss: 0.1215\n",
            "Epoch [99/100], Step [100/196], Fine Loss: 0.1207, Coarse Loss: 0.0563,  Total Loss: 0.0885\n",
            "Epoch [100/100], Step [100/196], Fine Loss: 0.0752, Coarse Loss: 0.0547,  Total Loss: 0.0649\n"
          ]
        }
      ]
    }
  ]
}
